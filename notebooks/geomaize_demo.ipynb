{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extractions from OpenEO\n",
    "\n",
    "To run the extractions, you need an account in the [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/WorldCereal/prometheo.git --quiet\n",
    "!pip install git+https://github.com/ScaleAGData/scaleag-vito.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from loguru import logger\n",
    "from scaleagdata_vito.presto.datasets_prometheo import ScaleAgDataset\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import generate_input_for_extractions, extract\n",
    "from scaleagdata_vito.presto.presto_df import load_dataset\n",
    "from scaleagdata_vito.presto.utils import train_test_val_split, finetune_on_task, load_finetuned_model, evaluate_finetuned_model, get_pretrained_model_url\n",
    "from scaleagdata_vito.presto.inference import PrestoPredictor, reshape_result, plot_results\n",
    "from scaleagdata_vito.utils.map import ui_map\n",
    "from scaleagdata_vito.utils.dateslider import date_slider\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import collect_inputs_for_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start...\n",
    "\n",
    "**Check your data!** Investigate validity of geometries uniqueness of sample IDs, presence of outliers and so on before starting the extraction. Achieving good performance making use of a limited amount of data is a challening task per se. Therefore, **the quality of your data will greatly impact your final results.**\n",
    "\n",
    "Data requirements:\n",
    "- Points or Polygons (will be aggregated in points)\n",
    "- Lat-Lon (crs:4326) \n",
    "- Format: parquet, GeoJSON, shapefile, GPKG\n",
    "For each geometry:\n",
    "- Date (if available) \n",
    "- Unique ID\n",
    "- Annotations\n",
    "\n",
    "Good practice:\n",
    "\n",
    "Remove polygons close to borders (e.g. apply buffer) to ensure data are contained in the field\n",
    "If the annotations are accurate, point geometries should be preferred. However, especially in regression tasks (i.e., continuous output values) such us yield estimation the target values might be noisy. In that case, we recommend subdividing the polygons in subfields of 20m x 20m (to cover more measurements) and computing the median yield for a smoother and more reliable target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess data correctness before launching the OpenEO jobs \n",
    "You can run some checks on your input file to make sure they are suitable to run the extractions successfully. In particular, it is important to check the validity of the geometries and, ideally, to also have a column containing a unique id for each sample.\n",
    "\n",
    "In case of invalid geometries, you will be provided with both the dataframe with the failing polygons to be fixed and the one with valid geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_id(df_path, unique_id):\n",
    "    df = gpd.read_file(df_path)\n",
    "    if df[unique_id].nunique() != df.shape[0]:\n",
    "        logger.info(\"IDs are not unique!\")\n",
    "        return df[df[unique_id].duplicated(keep=False)]\n",
    "    else:\n",
    "        logger.info(\"IDs are unique\")\n",
    "        return None\n",
    "\n",
    "def check_valid_geometry(df):\n",
    "    if isinstance(df, str):\n",
    "        df = gpd.read_file(df)\n",
    "    df_invalid = df[~df.geometry.is_valid]\n",
    "    # Assessing wheather some invalid geometries are present\n",
    "    if len(df_invalid) > 0:\n",
    "        # 1) some invalid geometries are present. Attempt fixing them\n",
    "        df['geometry'] = df.geometry.buffer(0)\n",
    "        df_invalid = df[~df.geometry.is_valid]\n",
    "        df_valid = df[df.geometry.is_valid]\n",
    "        if len(df_invalid) > 0:\n",
    "            # 2) Still some invalid geometries are present. Return them\n",
    "            logger.info(\"Invalid geometries found! Returning invalid geometries\")\n",
    "            return df_invalid, df_valid\n",
    "        else:\n",
    "            # All geometries are now valid. Return fixed dataframe and empty dataframe for invalid geometries\n",
    "            logger.info(\"Fixed some invalid geometries. All geometries are now valid\")\n",
    "            return gpd.GeoDataFrame(), df\n",
    "    else:\n",
    "        # All geometries are valid. Return empty dataframe for invalid geometries\n",
    "        logger.info(\"All geometries are valid\")\n",
    "        return gpd.GeoDataFrame(), df\n",
    "\n",
    "def _save(save_to, original_file_path, df, suffix=''):\n",
    "    if suffix!='':\n",
    "        filename = Path(save_to) / f\"{Path(original_file_path).stem}_{suffix}.geojson\"\n",
    "    else:\n",
    "        filename = Path(save_to) / f\"{Path(original_file_path).stem}.geojson\"\n",
    "    logger.info(f\"Saving invalid geometries to {filename}\")\n",
    "    Path(save_to).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:15:54.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcheck_valid_geometry\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mAll geometries are valid\u001b[0m\n",
      "\u001b[32m2025-07-17 15:15:54.430\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcheck_unique_id\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mIDs are unique\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\"\n",
    "invalid_geom, valid_geom = check_valid_geometry(input_file)\n",
    "non_unique_ids = check_unique_id(input_file, unique_id=\"Field_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:15:56.972\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m_save\u001b[0m:\u001b[36m38\u001b[0m - \u001b[1mSaving invalid geometries to /home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save files after geometry validity check. If invalid geometries are present, save them to a separate file\n",
    "if len(invalid_geom) > 0:\n",
    "    _save(\n",
    "        save_to=\"/home/giorgia/Private/data/geomaize/invalid/\",\n",
    "        original_file_path=input_file,\n",
    "        df=invalid_geom,\n",
    "        suffix='',\n",
    "    )\n",
    "\n",
    "# save valid geometries to a separate file\n",
    "_save(\n",
    "    save_to=\"/home/giorgia/Private/data/geomaize/correct/\",\n",
    "    original_file_path=input_file,\n",
    "    df=valid_geom,\n",
    "    suffix='',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements for running the extractions\n",
    "- Account in [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/). You can sign up for free and have a monthly availability of 10000 credits.\n",
    "- A dataset with valid geometries (Points or Polygons) in lat-lon projection.\n",
    "- Preferably a dataset with unique IDs per sample \n",
    "- A labelled dataset. Not required for the extraction process, but for the following fine-tuning steps.\n",
    "\n",
    "#### EO data extractions\n",
    "In this first step, we extract for each sample in your dataset the required EO time series from CDSE using OpenEO.\n",
    "For running the job, the user should indicate the following job_dictionary fields:\n",
    "\n",
    "```python\n",
    "    job_params = dict(\n",
    "        output_folder=..., # where to save the extracted dataset\n",
    "        input_df=..., # input georeferenced dataset to run the extractions for \n",
    "        start_date=..., # string indicating from which date to extract data  \n",
    "        end_date=..., # string indicating until which date to extract the data \n",
    "        unique_id_column=..., # name of the column in the input_df containing the unique ID of the samples  \n",
    "        composite_window=..., # \"month\" or \"dekad\" are supported. Default is \"dekad\"\n",
    "    )\n",
    "```\n",
    "in particular:\n",
    "- If the `date` information associated with the label is provided, the `start_date` of the time-series is automatically set to 9 months before the date, whereas the `end_date` is set to 9 months after. If `date` is not available, the user needs to manually indicate the desired `start_date` and `end_date` for the extractions.\n",
    "- `composite_window` indicates the time-series granularity, which can be dekadal or monthly. \n",
    "  - `dekad`: each time step in the extracted time series corresponds to a mean-compositing operation on 10-days acquisitions. Accordingly with the start and end date, each month will be covered by 3 time steps which, by default, correspond to the 1st, 11th and 21th of the month. \n",
    "  - `month`: each time step in the extracted time series corresponds to a mean-compositing operation on 30-days acquisitions. Each month will be covered by 1 time step which, by default, correspond to the 1st of the month.\n",
    "\n",
    "The following decadal/monthly time series will be extracted for the indicated time range:\n",
    "\n",
    "- Sentinel-2 L2A data (all bands)\n",
    "- Sentinel-1 VH and VV\n",
    "- Average air temperature and precipitation sum derived from AgERA5\n",
    "- Slope and elevation from Copernicus DEM\n",
    "\n",
    "Presto accepts 1D time-series. Therefore, if Polygons are provided for the extractions, the latter are spatially aggregated in points which will correspond to the centroid lat lon geolocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Run extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "start_date=\"2021-07-01\"\n",
    "end_date=\"2021-10-31\"\n",
    "composite_window=\"dekad\"\n",
    "unique_id_column=\"Field_id\"\n",
    "input_df=\"/home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\"\n",
    "output_folder=\"/home/giorgia/Private/data/geomaize/extractions_North_Ghana/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_id</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Area(acre)</th>\n",
       "      <th>COMMUNITY</th>\n",
       "      <th>REGION</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "      <th>Type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2042JIM</td>\n",
       "      <td>maize</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>JIMLE</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.50654 9.39554 154.28076, -0.506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3013BAD</td>\n",
       "      <td>Maize</td>\n",
       "      <td>0.763880</td>\n",
       "      <td>BADIGU</td>\n",
       "      <td>Northern</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.69446 9.5687 0, -0.69448 9.5686...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4034CHI</td>\n",
       "      <td>Maize</td>\n",
       "      <td>2.562025</td>\n",
       "      <td>Chahandayili</td>\n",
       "      <td>Savanna</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.4552 9.22023 0, -0.45521 9.2203...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4019CHI</td>\n",
       "      <td>Maize</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>Chehandayili</td>\n",
       "      <td>Savanna</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.44707 9.22686 0, -0.44714 9.226...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4037CHI</td>\n",
       "      <td>Maize</td>\n",
       "      <td>1.897256</td>\n",
       "      <td>Chahandayili</td>\n",
       "      <td>Savanna</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.44376 9.22794 0, -0.4438 9.2280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>33036TAM</td>\n",
       "      <td>maize</td>\n",
       "      <td>1.396200</td>\n",
       "      <td>Tampion</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.65362 9.58636 0, -0.65343 9.586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>33048TAM</td>\n",
       "      <td>maize</td>\n",
       "      <td>1.314670</td>\n",
       "      <td>Tampion</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.64357 9.5811 0, -0.64336 9.5810...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>33051TAM</td>\n",
       "      <td>maize</td>\n",
       "      <td>1.828420</td>\n",
       "      <td>Tampion</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.64344 9.57268 0, -0.64331 9.572...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>33061TAM</td>\n",
       "      <td>maize</td>\n",
       "      <td>2.347370</td>\n",
       "      <td>Tampion</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.61294 9.56017 0, -0.61288 9.560...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>33082TAM</td>\n",
       "      <td>maize</td>\n",
       "      <td>1.725220</td>\n",
       "      <td>Tampion</td>\n",
       "      <td>NORTHERN</td>\n",
       "      <td>July-October</td>\n",
       "      <td>2021</td>\n",
       "      <td>Mono Cropping</td>\n",
       "      <td>POLYGON Z ((-0.65278 9.55732 0, -0.65265 9.557...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Field_id   Crop  Area(acre)     COMMUNITY    REGION        Season  Year  \\\n",
       "0     2042JIM  maize    1.580000         JIMLE  NORTHERN  July-October  2021   \n",
       "1     3013BAD  Maize    0.763880        BADIGU  Northern  July-October  2021   \n",
       "2     4034CHI  Maize    2.562025  Chahandayili   Savanna  July-October  2021   \n",
       "3     4019CHI  Maize    0.877704  Chehandayili   Savanna  July-October  2021   \n",
       "4     4037CHI  Maize    1.897256  Chahandayili   Savanna  July-October  2021   \n",
       "..        ...    ...         ...           ...       ...           ...   ...   \n",
       "178  33036TAM  maize    1.396200       Tampion  NORTHERN  July-October  2021   \n",
       "179  33048TAM  maize    1.314670       Tampion  NORTHERN  July-October  2021   \n",
       "180  33051TAM  maize    1.828420       Tampion  NORTHERN  July-October  2021   \n",
       "181  33061TAM  maize    2.347370       Tampion  NORTHERN  July-October  2021   \n",
       "182  33082TAM  maize    1.725220       Tampion  NORTHERN  July-October  2021   \n",
       "\n",
       "              Type                                           geometry  \n",
       "0    Mono Cropping  POLYGON Z ((-0.50654 9.39554 154.28076, -0.506...  \n",
       "1    Mono Cropping  POLYGON Z ((-0.69446 9.5687 0, -0.69448 9.5686...  \n",
       "2    Mono Cropping  POLYGON Z ((-0.4552 9.22023 0, -0.45521 9.2203...  \n",
       "3    Mono Cropping  POLYGON Z ((-0.44707 9.22686 0, -0.44714 9.226...  \n",
       "4    Mono Cropping  POLYGON Z ((-0.44376 9.22794 0, -0.4438 9.2280...  \n",
       "..             ...                                                ...  \n",
       "178  Mono Cropping  POLYGON Z ((-0.65362 9.58636 0, -0.65343 9.586...  \n",
       "179  Mono Cropping  POLYGON Z ((-0.64357 9.5811 0, -0.64336 9.5810...  \n",
       "180  Mono Cropping  POLYGON Z ((-0.64344 9.57268 0, -0.64331 9.572...  \n",
       "181  Mono Cropping  POLYGON Z ((-0.61294 9.56017 0, -0.61288 9.560...  \n",
       "182  Mono Cropping  POLYGON Z ((-0.65278 9.55732 0, -0.65265 9.557...  \n",
       "\n",
       "[183 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check input data structure \n",
    "gpd.read_file(input_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 15:16:20,198|extraction_pipeline|INFO:  Loading input dataframe from /home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson.\n",
      "2025-07-17 15:16:20,226|extraction_pipeline|INFO:  Setting up the extraction functions.\n",
      "2025-07-17 15:16:20,227|extraction_pipeline|INFO:  Initializing the job manager.\n",
      "2025-07-17 15:16:20,228|extraction_pipeline|INFO:  Launching the jobs manager.\n",
      "2025-07-17 15:16:20,228|openeo_gfmap.manager|INFO:  Starting ThreadPoolExecutor with 4 workers.\n",
      "2025-07-17 15:16:20,229|openeo_gfmap.manager|INFO:  Creating and running jobs.\n",
      "2025-07-17 15:16:20,239|openeo_gfmap.manager|INFO:  Quitting job tracking & waiting for last post-job actions to finish.\n",
      "2025-07-17 15:16:20,240|openeo_gfmap.manager|INFO:  Exiting ThreadPoolExecutor.\n",
      "2025-07-17 15:16:20,240|openeo_gfmap.manager|INFO:  All jobs finished running.\n",
      "2025-07-17 15:16:20,240|openeo_gfmap.manager|INFO:  STAC was disabled, skipping generation of the catalogue.\n",
      "2025-07-17 15:16:20,241|extraction_pipeline|INFO:  Extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset will be extracted, it can be loaded with the `load_dataset` function by specifying the path where the `.parquet` files have been downloaded. Moreover, the following manipulations of the dataset are also possible:\n",
    "\n",
    "- `window_of_interest`: the user can specify a time window of interest out of the whole available time-series. `start_date` and `end_date` should be provided as strings in a list.\n",
    "- `use_valid_time`: the user might want to define the window of interest based on the `date` the label is associated with. If so, also `required_min_timesteps` should be provided\n",
    "- `buffer_window`: buffers the `start_date` and `end_date` by the number of time steps here specified  \n",
    "\n",
    "In the following cell, we load the extracted dataset for 1 year of data.\n",
    "\n",
    "**NOTE:** this code currently assumes that we are dealing with 1 year of data falling in the same time period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Presto datasets initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.00it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.88it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/giorgia/Private/data/geomaize/extractions_2021\"\n",
    "valid_path = \"/home/giorgia/Private/data/geomaize/extractions_2022\"\n",
    "test_path = \"/home/giorgia/Private/data/geomaize/extractions_2023\"\n",
    "\n",
    "train_df = load_dataset(\n",
    "    train_path,\n",
    "    window_of_interest=[start_date, end_date],\n",
    "    composite_window=composite_window\n",
    "    )\n",
    "\n",
    "val_df = load_dataset(\n",
    "    valid_path,\n",
    "    window_of_interest=[start_date, end_date],\n",
    "    composite_window=composite_window\n",
    "    )\n",
    "\n",
    "test_df = load_dataset(\n",
    "    test_path,\n",
    "    window_of_interest=[start_date, end_date],\n",
    "    composite_window=composite_window\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the parameters needed for initializing presto datasets for the specific task:\n",
    "- `num_timesteps`: can be inferred by the max number of the `available_timesteps` \n",
    "- `target_name`: name of the column containing the target data\n",
    "- `upper_bound` and `lower_bound`: these should be set to the min and max of the distribution. Therefore, it is important to get rid of potential outlaiers beforehand.\n",
    "\n",
    "**NOTE:** upper and lower bounds are also used to normalize the targets during the training process. Therefore it is important to keep track of such values to convert the predictions to the original units in the inference step!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the parameters needed for initializing presto datasets for the specific task:\n",
    "- `num_timesteps`: can be inferred by the max number of the `available_timesteps` \n",
    "- `target_name`: name of the column containing the target data\n",
    "- `upper_bound` and `lower_bound`: these should be set to the min and max of the distribution. Therefore, it is important to get rid of potential outlaiers beforehand.\n",
    "\n",
    "**NOTE:** upper and lower bounds are also used to normalize the targets during the training process. Therefore it is important to keep track of such values to convert the predictions to the original units in the inference step!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution to check for outliers to exclude if needed\n",
    "num_timesteps = train_df.available_timesteps.max()\n",
    "task_type = \"regression\"\n",
    "target_name=\"Yield kg/H\"\n",
    "upper_bound = train_df[target_name].max() \n",
    "lower_bound = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Initialize the training, validation and test datasets objects to be used for training Presto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:17:02.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.datasets_prometheo\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:17:02.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.datasets_prometheo\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n",
      "\u001b[32m2025-07-17 15:17:02.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.datasets_prometheo\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m140\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_ds = ScaleAgDataset(\n",
    "    dataframe=train_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=train_df[target_name].max(),\n",
    "    lower_bound=lower_bound,\n",
    ")\n",
    "\n",
    "# best practice is for the target_name to be consistent across datasets. \n",
    "# if so this only need to be defined in the cell above, and the string passed to the following datasets\n",
    "# as target_name should be replaced by target_name (i.e. target_name=\"Kg/ha\" -> target_name=target_name)\n",
    "\n",
    "val_ds = ScaleAgDataset(\n",
    "    dataframe=val_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=\"Kg/ha\",\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")\n",
    "\n",
    "test_ds = ScaleAgDataset(\n",
    "    dataframe=test_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=\"Kg/ha\",\n",
    "    composite_window=composite_window,\n",
    "    upper_bound=upper_bound,\n",
    "    lower_bound=lower_bound,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Presto Finetuning\n",
    "\n",
    "In this section Presto will be Fine-Tuned in a supervised way for the target downstream task. first we set up the following experiment parameters:\n",
    "\n",
    "- `output_dir` : where to dave the model \n",
    "- `experiment_name` : the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model Hyperparameters\n",
    "model_output_dir = Path(\"/home/giorgia/Private/data/geomaize/models/\")\n",
    "experiment_name = \"presto-ss-wc-10D-ft-dek_geomaize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:54.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m:\u001b[36mfinetune_on_task\u001b[0m:\u001b[36m215\u001b[0m - \u001b[1mFinetuning the model on regression task\u001b[0m\n",
      "\u001b[32m2025-07-17 15:21:54.771\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_setup\u001b[0m:\u001b[36m237\u001b[0m - \u001b[1mUsing output dir: /data/users/Private/giorgia/data/geomaize/models\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800f8a1e91924a21ae90cd50b49ea2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finetuning:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8996cc345204477faffb23c85ad3d76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:58.669\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 1/50: Epoch 1/50 | Train Loss: 0.0506 | Val Loss: 0.1937 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1937 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a855bb365ee4c509191383c22b7e06d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:58.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 2/50: Epoch 2/50 | Train Loss: 0.0468 | Val Loss: 0.1816 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1816 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8a665a940a4894b335f7a6b026d7de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:59.021\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 3/50: Epoch 3/50 | Train Loss: 0.0441 | Val Loss: 0.1710 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1710 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747585372c2c4fc39fb00729b3e0e419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:59.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 4/50: Epoch 4/50 | Train Loss: 0.0424 | Val Loss: 0.1622 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1622 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030446e92dfe42fa8646768dc83097fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:59.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 5/50: Epoch 5/50 | Train Loss: 0.0416 | Val Loss: 0.1552 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1552 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e077397c582843cdb9df77776a10afbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:59.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 6/50: Epoch 6/50 | Train Loss: 0.0414 | Val Loss: 0.1500 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1500 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ddaed70e9e4b30958566cfba3e21c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:59.676\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 7/50: Epoch 7/50 | Train Loss: 0.0416 | Val Loss: 0.1465 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1465 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec9746e513e4f1ab0e5e4f465ed5f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:59.830\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 8/50: Epoch 8/50 | Train Loss: 0.0418 | Val Loss: 0.1445 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1445 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f9e0d387c04d559a6733040c2d1051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:21:59.979\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 9/50: Epoch 9/50 | Train Loss: 0.0419 | Val Loss: 0.1435 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1435 (improved)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d15d5a074984088b8ce38ca9806d356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:22:00.129\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 10/50: Epoch 10/50 | Train Loss: 0.0418 | Val Loss: 0.1436 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1435 (no improvement for 1 epochs)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3c5fa1adb34cb1a87bcb301c697b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:22:00.468\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m192\u001b[0m - \u001b[1mPROGRESS after Epoch 11/50: Epoch 11/50 | Train Loss: 0.0414 | Val Loss: 0.1443 | Val Acc: 0.279 | Val Macro F1: 0.218 | Best Loss: 0.1435 (no improvement for 2 epochs)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729e2d0895dd413fa11e36559c7d380a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-17 15:22:00.641\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36m_train_loop\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1mEarly stopping!\u001b[0m\n",
      "\u001b[32m2025-07-17 15:22:00.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m:\u001b[36mrun_finetuning\u001b[0m:\u001b[36m355\u001b[0m - \u001b[1mFinetuning done\u001b[0m\n",
      "\u001b[32m2025-07-17 15:22:00.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m:\u001b[36mevaluate_finetuned_model\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mEvaluating the finetuned model on regression task\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 1164.3414984445071,\n",
       " 'R2_score': -0.3026844263076782,\n",
       " 'explained_var_score': -0.06476986408233643,\n",
       " 'MAPE': 0.39721816778182983}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the model with finetuning head starting from the pretrained model\n",
    "finetuned_model = finetune_on_task(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    pretrained_model_path=get_pretrained_model_url(composite_window=composite_window),\n",
    "    output_dir=model_output_dir, \n",
    "    experiment_name=experiment_name,\n",
    "    num_workers=0,\n",
    "    )\n",
    "evaluate_finetuned_model(finetuned_model, test_ds, num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Inference using Fine-Tuned end-to-end Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we apply the fine tuned model to generate a yield map on an unseen area. \n",
    "We need to indicate the spatial and temporal extent. The 2 cells below, offer a simple way for the user to provide these information and perform once again the extraction from CDSE of the EO time-series required by Presto. \n",
    "We also need to indicate the `output_dir` of where to save the datacube of the extraction, its `output_filename` and the `composite_window` which will be the same as used for finetuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = ui_map(area_limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 1 year of data\n",
    "slider = date_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/home/giorgia/Private/data/geomaize/regression\")\n",
    "output_filename = \"inference_area\"\n",
    "inference_file = output_dir / f\"{output_filename}.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_inputs_for_inference(\n",
    "    spatial_extent=map.get_extent(),\n",
    "    temporal_extent=slider.get_processing_period(),\n",
    "    output_path=output_dir,\n",
    "    output_filename=f\"{output_filename}.nc\",\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the datacube has been extracted, we can perform the inference task using the finetuned model and visualize the predicted map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_file = output_dir / \"inference_area.nc\"\n",
    "mask_path = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = load_finetuned_model(model_output_dir / experiment_name, task_type=task_type)\n",
    "presto_model = PrestoPredictor(\n",
    "    model=finetuned_model,\n",
    "    batch_size=50,\n",
    "    task_type=task_type,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "predictions = presto_model.predict(inference_file, upper_bound=upper_bound, lower_bound=lower_bound, mask_path=mask_path)\n",
    "predictions_map = reshape_result(predictions, path_to_input_file=inference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(prob_map=predictions_map, path_to_input_file=inference_file, task=task_type, ts_index=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
