{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extractions from OpenEO\n",
    "\n",
    "To run the extractions, you need an account in the [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/WorldCereal/prometheo.git@scaleag_augmentations --quiet\n",
    "!pip install git+https://github.com/ScaleAGData/scaleag-vito.git@prometheo-integration --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import (\n",
    "    generate_input_for_extractions,\n",
    "    extract\n",
    ")\n",
    "from scaleagdata_vito.presto.presto_df import load_dataset\n",
    "from scaleagdata_vito.presto.utils import evaluate_finetuned_model\n",
    "from prometheo.datasets.scaleag import ScaleAgDataset\n",
    "from prometheo import finetune\n",
    "from prometheo.finetune import Hyperparams\n",
    "from prometheo.models.presto import param_groups_lrd\n",
    "from prometheo.models.presto.wrapper import PretrainedPrestoWrapper, load_pretrained\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from scaleagdata_vito.presto.utils import evaluate_downstream_model, get_encodings\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess data correctness before launching the OpenEO jobs \n",
    "You can run some checks on your input file to make sure they are suitable to run the extractions successfully. In particular, it is important to check the validity of the geometries and, ideally, to also have a column containing a unique id for each sample.\n",
    "\n",
    "In case of invalid geometries, you will be provided with both the dataframe with the failing polygons to be fixed and the one with valid geometries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_id(df_path, unique_id):\n",
    "    df = gpd.read_file(df_path)\n",
    "    if df[unique_id].nunique() != df.shape[0]:\n",
    "        logger.info(\"IDs are not unique!\")\n",
    "        return df[df[unique_id].duplicated(keep=False)]\n",
    "    else:\n",
    "        logger.info(\"IDs are unique\")\n",
    "        return None\n",
    "\n",
    "def check_valid_geometry(df):\n",
    "    if isinstance(df, str):\n",
    "        df = gpd.read_file(df)\n",
    "    df_invalid = df[~df.geometry.is_valid]\n",
    "    # Assessing wheather some invalid geometries are present\n",
    "    if len(df_invalid) > 0:\n",
    "        # 1) some invalid geometries are present. Attempt fixing them\n",
    "        df['geometry'] = df.geometry.buffer(0)\n",
    "        df_invalid = df[~df.geometry.is_valid]\n",
    "        df_valid = df[df.geometry.is_valid]\n",
    "        if len(df_invalid) > 0:\n",
    "            # 2) Still some invalid geometries are present. Return them\n",
    "            logger.info(\"Invalid geometries found! Returning invalid geometries\")\n",
    "            return df_invalid, df_valid\n",
    "        else:\n",
    "            # All geometries are now valid. Return fixed dataframe and empty dataframe for invalid geometries\n",
    "            logger.info(\"Fixed some invalid geometries. All geometries are now valid\")\n",
    "            return gpd.GeoDataFrame(), df\n",
    "    else:\n",
    "        # All geometries are valid. Return empty dataframe for invalid geometries\n",
    "        logger.info(\"All geometries are valid\")\n",
    "        return gpd.GeoDataFrame(), df\n",
    "\n",
    "def _save(save_to, original_file_path, df, suffix=''):\n",
    "    if suffix!='':\n",
    "        filename = Path(save_to) / f\"{Path(original_file_path).stem}_{suffix}.geojson\"\n",
    "    else:\n",
    "        filename = Path(save_to) / f\"{Path(original_file_path).stem}.geojson\"\n",
    "    logger.info(f\"Saving invalid geometries to {filename}\")\n",
    "    Path(save_to).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:19:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m - \u001b[1mAll geometries are valid\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:19:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m - \u001b[1mIDs are unique\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\"\n",
    "invalid_geom, valid_geom = check_valid_geometry(input_file)\n",
    "non_unique_ids = check_unique_id(input_file, unique_id=\"Field_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:19:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m - \u001b[1mSaving invalid geometries to /home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save files after geometry validity check. If invalid geometries are present, save them to a separate file\n",
    "if len(invalid_geom) > 0:\n",
    "    _save(\n",
    "        save_to=\"/home/giorgia/Private/data/geomaize/invalid/\",\n",
    "        original_file_path=input_file,\n",
    "        df=invalid_geom,\n",
    "        suffix='',\n",
    "    )\n",
    "\n",
    "# save valid geometries to a separate file\n",
    "_save(\n",
    "    save_to=\"/home/giorgia/Private/data/geomaize/correct/\",\n",
    "    original_file_path=input_file,\n",
    "    df=valid_geom,\n",
    "    suffix='',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide job instructions and start OpenEO extractions\n",
    "\n",
    "1) Indicate the following fields in order to guide the extraction\n",
    "2) In the cell below you will be asked for authentication and be provided with a link. click on the link and login with your CDSE credentials.  \n",
    "3) Once the extraction process will be over, you will find your extracted dataset in the output folder you indicated. You can load it by running `load_dataset` as shown below\n",
    "\n",
    "    ```python\n",
    "    job_params = dict(\n",
    "        output_folder=..., # where to save the extracted dataset\n",
    "        input_df=..., # input georeferenced dataset to run the extractions for \n",
    "        start_date=..., # string indicating from which date to extract data  \n",
    "        end_date=..., # string indicating until which date to extract the data \n",
    "        unique_id_column=..., # name of the column in the input_df containing the unique ID of the samples  \n",
    "        composite_window=..., # \"month\" or \"dekad\" are supported. Default is \"dekad\"\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 10:19:58,193|extraction_pipeline|INFO:  Loading input dataframe from /home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson.\n",
      "2025-03-17 10:19:58,228|extraction_pipeline|INFO:  Preparing the job dataframe.\n",
      "2025-03-17 10:19:58,229|extraction_pipeline|INFO:  Performing splitting by s2 grid...\n",
      "/data/users/Private/giorgia/git/openeo-gfmap/src/openeo_gfmap/manager/job_splitters.py:113: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  polygons[\"centroid\"] = polygons.geometry.centroid\n",
      "2025-03-17 10:19:58,460|extraction_pipeline|INFO:  Dataframes split to jobs, creating the job dataframe...\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:143: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:144: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:143: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:144: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:143: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:144: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      " 60%|██████    | 3/5 [00:00<00:00, 22.12it/s]/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:143: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:144: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:143: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lat\"] = job.geometry.centroid.y\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/scaleagdata_vito/openeo/extract_sample_scaleag.py:144: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  job[\"lon\"] = job.geometry.centroid.x\n",
      "100%|██████████| 5/5 [00:00<00:00, 24.24it/s]\n",
      "2025-03-17 10:19:58,671|extraction_pipeline|INFO:  Job dataframe created with 5 jobs.\n",
      "2025-03-17 10:19:58,672|extraction_pipeline|INFO:  Setting up the extraction functions.\n",
      "2025-03-17 10:19:58,673|extraction_pipeline|INFO:  Initializing the job manager.\n",
      "2025-03-17 10:19:58,676|extraction_pipeline|INFO:  Launching the jobs manager.\n",
      "2025-03-17 10:19:58,677|openeo_gfmap.manager|INFO:  Starting ThreadPoolExecutor with 4 workers.\n",
      "2025-03-17 10:19:58,678|openeo_gfmap.manager|INFO:  Creating and running jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/openeo/rest/connection.py:1266: UserWarning: SENTINEL2_L2A property filtering with properties that are undefined in the collection metadata (summaries): tileId.\n",
      "  return DataCube.load_collection(\n",
      "2025-03-17 10:20:04,802 - openeo_gfmap.utils - INFO - Selected orbit state: ASCENDING. Reason: Only orbit fully covering the requested area.\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/openeo/rest/connection.py:1266: UserWarning: SENTINEL2_L2A property filtering with properties that are undefined in the collection metadata (summaries): tileId.\n",
      "  return DataCube.load_collection(\n",
      "2025-03-17 10:20:31,820 - openeo_gfmap.utils - INFO - Selected orbit state: ASCENDING. Reason: Only orbit fully covering the requested area.\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/openeo/rest/connection.py:1266: UserWarning: SENTINEL2_L2A property filtering with properties that are undefined in the collection metadata (summaries): tileId.\n",
      "  return DataCube.load_collection(\n",
      "2025-03-17 10:23:05,974 - openeo_gfmap.utils - INFO - Selected orbit state: ASCENDING. Reason: Only orbit fully covering the requested area.\n",
      "2025-03-17 10:25:36,885|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-25031709203540c18f5f1da581de617c\n",
      "2025-03-17 10:25:36,945|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-03-17 10:25:36,946|openeo_gfmap.manager|INFO:  Job j-25031709203540c18f5f1da581de617c and post job action finished successfully.\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/openeo/rest/connection.py:1266: UserWarning: SENTINEL2_L2A property filtering with properties that are undefined in the collection metadata (summaries): tileId.\n",
      "  return DataCube.load_collection(\n",
      "2025-03-17 10:25:43,519 - openeo_gfmap.utils - INFO - Selected orbit state: ASCENDING. Reason: Only orbit fully covering the requested area.\n",
      "2025-03-17 10:32:17,680|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-25031709230942c49f2b698a2f9ce4ad\n",
      "2025-03-17 10:32:17,763|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-03-17 10:32:17,765|openeo_gfmap.manager|INFO:  Job j-25031709230942c49f2b698a2f9ce4ad and post job action finished successfully.\n",
      "/home/giorgia/.conda/envs/veg/lib/python3.10/site-packages/openeo/rest/connection.py:1266: UserWarning: SENTINEL2_L2A property filtering with properties that are undefined in the collection metadata (summaries): tileId.\n",
      "  return DataCube.load_collection(\n",
      "2025-03-17 10:32:23,693 - openeo_gfmap.utils - INFO - Selected orbit state: ASCENDING. Reason: Only orbit fully covering the requested area.\n",
      "2025-03-17 10:33:44,346|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-250317092546473e84d6bd5b25392108\n",
      "2025-03-17 10:33:44,412|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-03-17 10:33:44,414|openeo_gfmap.manager|INFO:  Job j-250317092546473e84d6bd5b25392108 and post job action finished successfully.\n",
      "2025-03-17 10:37:47,134|openeo_gfmap.manager|INFO:  Parsed item timeseries.parquet from job j-2503170932254066bca19927eb495091\n",
      "2025-03-17 10:37:47,189|openeo_gfmap.manager|INFO:  Adding 1 items to the STAC collection...\n",
      "2025-03-17 10:37:47,190|openeo_gfmap.manager|INFO:  Job j-2503170932254066bca19927eb495091 and post job action finished successfully.\n",
      "2025-03-17 10:38:47,264|openeo_gfmap.manager|INFO:  Quitting job tracking & waiting for last post-job actions to finish.\n",
      "2025-03-17 10:38:47,267|openeo_gfmap.manager|INFO:  Exiting ThreadPoolExecutor.\n",
      "2025-03-17 10:38:47,269|openeo_gfmap.manager|INFO:  All jobs finished running.\n",
      "2025-03-17 10:38:47,270|openeo_gfmap.manager|INFO:  STAC was disabled, skipping generation of the catalogue.\n",
      "2025-03-17 10:38:47,271|extraction_pipeline|INFO:  Extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Dataset Parameters\n",
    "task_type = \"regression\"\n",
    "start_date=\"2021-07-01\"\n",
    "end_date=\"2021-10-31\"\n",
    "composite_window=\"dekad\"\n",
    "unique_id_column=\"Field_id\"\n",
    "input_df=\"/home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\"\n",
    "output_folder=\"/home/giorgia/Private/data/geomaize/extractions_North_Ghana/\"\n",
    "\n",
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 18.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/giorgia/Private/data/geomaize/extractions_2021\"\n",
    "valid_path = \"/home/giorgia/Private/data/geomaize/extractions_2022\"\n",
    "test_path = \"/home/giorgia/Private/data/geomaize/extractions_2023\"\n",
    "\n",
    "train_df = load_dataset(\n",
    "    train_path,\n",
    "    composite_window=composite_window\n",
    "    )\n",
    "\n",
    "val_df = load_dataset(\n",
    "    valid_path,\n",
    "    composite_window=composite_window\n",
    "    )\n",
    "\n",
    "test_df = load_dataset(\n",
    "    test_path,\n",
    "    composite_window=composite_window\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:01:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.datasets.scaleag\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n",
      "\u001b[32m2025-03-17 10:01:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.datasets.scaleag\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n",
      "\u001b[32m2025-03-17 10:01:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.datasets.scaleag\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ds = ScaleAgDataset(\n",
    "    dataframe=train_df,\n",
    "    num_timesteps=train_df.available_timesteps.max(),\n",
    "    task_type=task_type,\n",
    "    target_name=\"Yield kg/H\",\n",
    "    compositing_window=composite_window,\n",
    ")\n",
    "\n",
    "val_ds = ScaleAgDataset(\n",
    "    dataframe=val_df,\n",
    "    num_timesteps=train_df.available_timesteps.max(),\n",
    "    task_type=task_type,\n",
    "    target_name=\"Kg/ha\",\n",
    "    compositing_window=composite_window,\n",
    ")\n",
    "\n",
    "test_ds = ScaleAgDataset(\n",
    "    dataframe=test_df,\n",
    "    num_timesteps=train_df.available_timesteps.max(),\n",
    "    task_type=task_type,\n",
    "    target_name=\"Kg/ha\",\n",
    "    compositing_window=composite_window,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning Hyperparameters for\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "num_workers = 2\n",
    "patience = 10\n",
    "pretrained_model_path = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/scaleagdata/models/presto-ss-wc_10D.pt\"\n",
    "output_dir = Path(\"/home/giorgia/Private/data/geomaize/models/\")\n",
    "experiment_name = \"presto-ss-wc-10D-ft-dek_geomaize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:02:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.utils\u001b[0m - \u001b[1mLogging setup complete. Logging to: /home/giorgia/Private/data/geomaize/models/logs/presto-ss-wc-10D-ft-dek_geomaize.log and console.\u001b[0m\n",
      "\u001b[32m2025-03-17 10:02:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m - \u001b[1mUsing output dir: /data/users/Private/giorgia/data/geomaize/models\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train metric: 0.048, Val metric: 0.131, Best Val Loss: 0.030 (no improvement for 9 epochs):  28%|██▊       | 14/50 [00:10<00:27,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:02:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m - \u001b[1mEarly stopping!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train metric: 0.048, Val metric: 0.131, Best Val Loss: 0.030 (no improvement for 9 epochs):  28%|██▊       | 14/50 [00:11<00:29,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:02:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m - \u001b[1mFinetuning done\u001b[0m\n",
      "\u001b[32m2025-03-17 10:02:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m - \u001b[1mEvaluating the finetuned model on regression task\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 1866.951171875,\n",
       " 'R2_score': -0.15574908256530762,\n",
       " 'explained_var_score': 0.041095733642578125,\n",
       " 'MAPE': 0.46740350127220154}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the model with finetuning head\n",
    "model = PretrainedPrestoWrapper(\n",
    "    num_outputs=train_ds.num_outputs,\n",
    "    regression=True if task_type == \"regression\" else False,\n",
    ")\n",
    "model = load_pretrained(model, pretrained_model_path, strict=False)\n",
    "\n",
    "# Reduce epochs for testing purposes\n",
    "hyperparams = Hyperparams(max_epochs=epochs, batch_size=batch_size, patience=patience, num_workers=num_workers, lr=lr)\n",
    "\n",
    "\n",
    "# set loss depending on the task type\n",
    "if task_type == \"regression\":\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif task_type == \"binary\":\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "parameters = param_groups_lrd(model)\n",
    "optimizer = AdamW(parameters, lr=hyperparams.lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "finetuned_model = finetune.run_finetuning(\n",
    "            model,\n",
    "            train_ds,\n",
    "            val_ds,\n",
    "            experiment_name=experiment_name,\n",
    "            output_dir=output_dir,\n",
    "            loss_fn=loss_fn,\n",
    "            hyperparams=hyperparams,\n",
    "        )\n",
    "\n",
    "evaluate_finetuned_model(\n",
    "    finetuned_model=finetuned_model,\n",
    "    test_ds=test_ds,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m - \u001b[1mComputing Presto encodings\u001b[0m\n",
      "\u001b[32m2025-03-17 10:07:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m - \u001b[1mFitting Catboost model on Presto encodings\u001b[0m\n",
      "\u001b[32m2025-03-17 10:07:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m - \u001b[1mEvaluating the finetuned model on regression task\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 1786.345340148278,\n",
       " 'R2_score': -0.058104321557600036,\n",
       " 'explained_var_score': 0.08963859747725222,\n",
       " 'MAPE': 1.053063968797406}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "cbm = cb.CatBoostRegressor(\n",
    "    random_state=3,\n",
    "    logging_level=\"Silent\",\n",
    "    loss_function=\"RMSE\",\n",
    ")\n",
    "logger.info(\"Computing Presto encodings\")\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n",
    "train_encodings, train_targets = get_encodings(train_dl, finetuned_model)\n",
    "logger.info(\"Fitting Catboost model on Presto encodings\")\n",
    "train_dataset = cb.Pool(train_encodings, train_targets)\n",
    "cbm.fit(train_dataset)\n",
    "\n",
    "evaluate_downstream_model(finetuned_model, cbm, test_ds, num_workers=num_workers, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RMSE': 53755341.12740031, 'R2_score': -0.04463906772080262, 'explained_var_score': 0.0005328617477075026, 'MAPE': 0.9037415384582995}\n"
     ]
    }
   ],
   "source": [
    "from scaleagdata_vito.presto.presto_utils_demo import revert_to_original_units\n",
    "from scaleagdata_vito.demo.utils import prepare_data_for_cb\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_percentage_error\n",
    "\n",
    "raw_cbm = cb.CatBoostRegressor(\n",
    "    random_state=3,\n",
    "    logging_level=\"Silent\",\n",
    "    loss_function=\"RMSE\",\n",
    ")\n",
    "\n",
    "train_x, train_y = prepare_data_for_cb(\n",
    "    train_df,\n",
    "    \"Yield kg/H\",\n",
    "    num_time_steps=train_df.available_timesteps.max(),\n",
    ")\n",
    "val_x, val_y = prepare_data_for_cb(\n",
    "    val_df, \n",
    "    \"Kg/ha\",\n",
    "    num_time_steps=train_df.available_timesteps.max(),\n",
    ")\n",
    "test_x, test_y = prepare_data_for_cb(\n",
    "    val_df, \n",
    "    \"Kg/ha\",\n",
    "    num_time_steps=train_df.available_timesteps.max(),\n",
    ")\n",
    "\n",
    "train_pool = cb.Pool(train_x, train_y)\n",
    "raw_cbm.fit(train_pool, eval_set=cb.Pool(val_x, val_y))\n",
    "\n",
    "preds = raw_cbm.predict(test_x)\n",
    "targets = revert_to_original_units(\n",
    "    test_y, upper_bound=test_ds.upper_bound, lower_bound=test_ds.lower_bound\n",
    ")\n",
    "preds = revert_to_original_units(\n",
    "    preds, upper_bound=test_ds.upper_bound, lower_bound=test_ds.lower_bound\n",
    ")\n",
    "print({\n",
    "    \"RMSE\": float(np.sqrt(mean_squared_error(targets, preds))),\n",
    "    \"R2_score\": float(r2_score(targets, preds)),\n",
    "    \"explained_var_score\": float(explained_variance_score(targets, preds)),\n",
    "    \"MAPE\": float(mean_absolute_percentage_error(targets, preds)),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
