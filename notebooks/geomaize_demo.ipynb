{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extractions from OpenEO\n",
    "\n",
    "To run the extractions, you need an account in the [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn --quiet\n",
    "!pip install git+https://github.com/WorldCereal/prometheo.git@scaleag_augmentations --quiet\n",
    "!pip install git+https://github.com/ScaleAGData/scaleag-vito.git@prometheo-integration --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import (\n",
    "    generate_input_for_extractions,\n",
    "    extract\n",
    ")\n",
    "from scaleagdata_vito.presto.presto_df import load_dataset\n",
    "# from scaleagdata_vito.presto.utils import evaluate_finetuned_model\n",
    "# from prometheo.datasets.scaleag import ScaleAgDataset\n",
    "# from prometheo import finetune\n",
    "# from prometheo.finetune import Hyperparams\n",
    "# from prometheo.models.presto import param_groups_lrd\n",
    "# from prometheo.models.presto.wrapper import PretrainedPrestoWrapper, load_pretrained\n",
    "# from torch import nn\n",
    "# from torch.optim import AdamW, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess data correctness before launching the OpenEO jobs \n",
    "You can run some checks on your input file to make sure they are suitable to run the extractions successfully. In particular, it is important to check the validity of the geometries and, ideally, to also have a column containing a unique id for each sample.\n",
    "\n",
    "In case of invalid geometries, you will be provided with both the dataframe with the failing polygons to be fixed and the one with valid geometries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_unique_id(df_path, unique_id):\n",
    "    df = gpd.read_file(df_path)\n",
    "    if df[unique_id].nunique() != df.shape[0]:\n",
    "        logger.info(\"IDs are not unique!\")\n",
    "        return df[df[unique_id].duplicated(keep=False)]\n",
    "    else:\n",
    "        logger.info(\"IDs are unique\")\n",
    "        return None\n",
    "\n",
    "def check_valid_geometry(df):\n",
    "    if isinstance(df, str):\n",
    "        df = gpd.read_file(df)\n",
    "    df_invalid = df[~df.geometry.is_valid]\n",
    "    # Assessing wheather some invalid geometries are present\n",
    "    if len(df_invalid) > 0:\n",
    "        # 1) some invalid geometries are present. Attempt fixing them\n",
    "        df['geometry'] = df.geometry.buffer(0)\n",
    "        df_invalid = df[~df.geometry.is_valid]\n",
    "        df_valid = df[df.geometry.is_valid]\n",
    "        if len(df_invalid) > 0:\n",
    "            # 2) Still some invalid geometries are present. Return them\n",
    "            logger.info(\"Invalid geometries found! Returning invalid geometries\")\n",
    "            return df_invalid, df_valid\n",
    "        else:\n",
    "            # All geometries are now valid. Return fixed dataframe and empty dataframe for invalid geometries\n",
    "            logger.info(\"Fixed some invalid geometries. All geometries are now valid\")\n",
    "            return gpd.GeoDataFrame(), df\n",
    "    else:\n",
    "        # All geometries are valid. Return empty dataframe for invalid geometries\n",
    "        logger.info(\"All geometries are valid\")\n",
    "        return gpd.GeoDataFrame(), df\n",
    "\n",
    "def _save(save_to, original_file_path, df, suffix=''):\n",
    "    if suffix!='':\n",
    "        filename = Path(save_to) / f\"{Path(original_file_path).stem}_{suffix}.geojson\"\n",
    "    else:\n",
    "        filename = Path(save_to) / f\"{Path(original_file_path).stem}.geojson\"\n",
    "    logger.info(f\"Saving invalid geometries to {filename}\")\n",
    "    Path(save_to).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-20 09:49:41.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcheck_valid_geometry\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mAll geometries are valid\u001b[0m\n",
      "\u001b[32m2025-03-20 09:49:41.850\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mcheck_unique_id\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mIDs are unique\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_file = \"/home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\"\n",
    "invalid_geom, valid_geom = check_valid_geometry(input_file)\n",
    "non_unique_ids = check_unique_id(input_file, unique_id=\"Field_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 10:19:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m - \u001b[1mSaving invalid geometries to /home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# save files after geometry validity check. If invalid geometries are present, save them to a separate file\n",
    "if len(invalid_geom) > 0:\n",
    "    _save(\n",
    "        save_to=\"/home/giorgia/Private/data/geomaize/invalid/\",\n",
    "        original_file_path=input_file,\n",
    "        df=invalid_geom,\n",
    "        suffix='',\n",
    "    )\n",
    "\n",
    "# save valid geometries to a separate file\n",
    "_save(\n",
    "    save_to=\"/home/giorgia/Private/data/geomaize/correct/\",\n",
    "    original_file_path=input_file,\n",
    "    df=valid_geom,\n",
    "    suffix='',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide job instructions and start OpenEO extractions\n",
    "\n",
    "1) Indicate the following fields in order to guide the extraction\n",
    "2) In the cell below you will be asked for authentication and be provided with a link. click on the link and login with your CDSE credentials.  \n",
    "3) Once the extraction process will be over, you will find your extracted dataset in the output folder you indicated. You can load it by running `load_dataset` as shown below\n",
    "\n",
    "    ```python\n",
    "    job_params = dict(\n",
    "        output_folder=..., # where to save the extracted dataset\n",
    "        input_df=..., # input georeferenced dataset to run the extractions for \n",
    "        start_date=..., # string indicating from which date to extract data  \n",
    "        end_date=..., # string indicating until which date to extract the data \n",
    "        unique_id_column=..., # name of the column in the input_df containing the unique ID of the samples  \n",
    "        composite_window=..., # \"month\" or \"dekad\" are supported. Default is \"dekad\"\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "task_type = \"regression\"\n",
    "start_date=\"2021-07-01\"\n",
    "end_date=\"2021-10-31\"\n",
    "composite_window=\"dekad\"\n",
    "unique_id_column=\"Field_id\"\n",
    "input_df=\"/home/giorgia/Private/data/geomaize/correct/Maize_North_Ghana_valid.geojson\"\n",
    "output_folder=\"/home/giorgia/Private/data/geomaize/extractions_North_Ghana/\"\n",
    "\n",
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/home/giorgia/Private/data/geomaize/extractions_2021\"\n",
    "valid_path = \"/home/giorgia/Private/data/geomaize/extractions_2022\"\n",
    "test_path = \"/home/giorgia/Private/data/geomaize/extractions_2023\"\n",
    "\n",
    "train_df = load_dataset(\n",
    "    train_path,\n",
    "    composite_window=composite_window\n",
    "    )\n",
    "\n",
    "val_df = load_dataset(\n",
    "    valid_path,\n",
    "    composite_window=composite_window\n",
    "    )\n",
    "\n",
    "test_df = load_dataset(\n",
    "    test_path,\n",
    "    composite_window=composite_window\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 14:18:17.396\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.datasets.scaleag\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m132\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 14:18:17.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.datasets.scaleag\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m132\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n",
      "\u001b[32m2025-03-17 14:18:17.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.datasets.scaleag\u001b[0m:\u001b[36mset_num_outputs\u001b[0m:\u001b[36m132\u001b[0m - \u001b[1mSetting number of outputs to 1 for regression task.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ds = ScaleAgDataset(\n",
    "    dataframe=train_df,\n",
    "    num_timesteps=train_df.available_timesteps.max(),\n",
    "    task_type=task_type,\n",
    "    target_name=\"Yield kg/H\",\n",
    "    compositing_window=composite_window,\n",
    ")\n",
    "\n",
    "val_ds = ScaleAgDataset(\n",
    "    dataframe=val_df,\n",
    "    num_timesteps=train_df.available_timesteps.max(),\n",
    "    task_type=task_type,\n",
    "    target_name=\"Kg/ha\",\n",
    "    compositing_window=composite_window,\n",
    ")\n",
    "\n",
    "test_ds = ScaleAgDataset(\n",
    "    dataframe=test_df,\n",
    "    num_timesteps=train_df.available_timesteps.max(),\n",
    "    task_type=task_type,\n",
    "    target_name=\"Kg/ha\",\n",
    "    compositing_window=composite_window,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning Hyperparameters for\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "num_workers = 2\n",
    "patience = 10\n",
    "pretrained_model_path = \"https://artifactory.vgt.vito.be/artifactory/auxdata-public/scaleagdata/models/presto-ss-wc_10D.pt\"\n",
    "output_dir = Path(\"/home/giorgia/Private/data/geomaize/models/\")\n",
    "experiment_name = \"presto-ss-wc-10D-ft-dek_geomaize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 14:18:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.utils\u001b[0m - \u001b[1mLogging setup complete. Logging to: /home/giorgia/Private/data/geomaize/models/logs/presto-ss-wc-10D-ft-dek_geomaize.log and console.\u001b[0m\n",
      "\u001b[32m2025-03-17 14:18:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m - \u001b[1mUsing output dir: /data/users/Private/giorgia/data/geomaize/models\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train metric: 0.066, Val metric: 0.179, Best Val Loss: 0.024 (no improvement for 9 epochs):  30%|███       | 15/50 [00:10<00:22,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 14:18:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m - \u001b[1mEarly stopping!\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train metric: 0.066, Val metric: 0.179, Best Val Loss: 0.024 (no improvement for 9 epochs):  30%|███       | 15/50 [00:11<00:26,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-17 14:18:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mprometheo.finetune\u001b[0m - \u001b[1mFinetuning done\u001b[0m\n",
      "\u001b[32m2025-03-17 14:18:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mscaleagdata_vito.presto.utils\u001b[0m - \u001b[1mEvaluating the finetuned model on regression task\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'RMSE': 2161.155029296875,\n",
       " 'R2_score': -0.5487079620361328,\n",
       " 'explained_var_score': -0.08014082908630371,\n",
       " 'MAPE': 0.38651517033576965}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the model with finetuning head\n",
    "model = PretrainedPrestoWrapper(\n",
    "    num_outputs=train_ds.num_outputs,\n",
    "    regression=True if task_type == \"regression\" else False,\n",
    ")\n",
    "model = load_pretrained(model, pretrained_model_path, strict=False)\n",
    "\n",
    "# Reduce epochs for testing purposes\n",
    "hyperparams = Hyperparams(max_epochs=epochs, batch_size=batch_size, patience=patience, num_workers=num_workers, lr=lr)\n",
    "\n",
    "\n",
    "# set loss depending on the task type\n",
    "if task_type == \"regression\":\n",
    "    loss_fn = nn.MSELoss()\n",
    "elif task_type == \"binary\":\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "parameters = param_groups_lrd(model)\n",
    "optimizer = AdamW(parameters, lr=hyperparams.lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "finetuned_model = finetune.run_finetuning(\n",
    "            model,\n",
    "            train_ds,\n",
    "            val_ds,\n",
    "            experiment_name=experiment_name,\n",
    "            output_dir=output_dir,\n",
    "            loss_fn=loss_fn,\n",
    "            hyperparams=hyperparams,\n",
    "        )\n",
    "\n",
    "evaluate_finetuned_model(\n",
    "    finetuned_model=finetuned_model,\n",
    "    test_ds=test_ds,\n",
    "    num_workers=num_workers,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "veg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
