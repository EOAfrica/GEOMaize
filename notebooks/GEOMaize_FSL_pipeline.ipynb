{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extractions from OpenEO\n",
    "\n",
    "To run the extractions, you need an account in the [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ScaleAGData/scaleag-vito.git@prometheo-integration --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from scaleagdata_vito.presto.datasets_prometheo import ScaleAgDataset\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import generate_input_for_extractions, extract\n",
    "from scaleagdata_vito.presto.presto_df import load_dataset\n",
    "from scaleagdata_vito.presto.utils import train_test_val_split, finetune_on_task, load_finetuned_model, evaluate_finetuned_model, get_pretrained_model_url\n",
    "from scaleagdata_vito.presto.inference import PrestoPredictor, reshape_result, plot_results\n",
    "from scaleagdata_vito.utils.map import ui_map\n",
    "from scaleagdata_vito.utils.dateslider import date_slider\n",
    "from scaleagdata_vito.openeo.extract_sample_scaleag import collect_inputs_for_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we start...\n",
    "\n",
    "**Check your data!** Investigate validity of geometries uniqueness of sample IDs, presence of outliers and so on before starting the extraction. Achieving good performance making use of a limited amount of data is a challening task per se. Therefore, **the quality of your data will greatly impact your final results.**\n",
    "\n",
    "Data requirements:\n",
    "- Points or Polygons (will be aggregated in points)\n",
    "- Lat-Lon (crs:4326) \n",
    "- Format: parquet, GeoJSON, shapefile, GPKG\n",
    "For each geometry:\n",
    "- Date (if available) \n",
    "- Unique ID\n",
    "- Annotations\n",
    "\n",
    "Good practice:\n",
    "\n",
    "Remove polygons close to borders (e.g. apply buffer) to ensure data are contained in the field\n",
    "If the annotations are accurate, point geometries should be preferred. However, especially in regression tasks (i.e., continuous output values) such us yield estimation the target values might be noisy. In that case, we recommend subdividing the polygons in subfields of 20m x 20m (to cover more measurements) and computing the median yield for a smoother and more reliable target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assess data correctness before launching the OpenEO jobs \n",
    "You can run some checks on your input file to make sure they are suitable to run the extractions successfully. In particular, it is important to check the validity of the geometries and, to also have a column containing a unique id for each sample. Do these checks beforehand by running the first section of the notebook `data_investigation.ipynb`\n",
    "\n",
    "##### Requirements for running the extractions\n",
    "- Account in [Copernicus Data Space Ecosystem (CDSE)](https://dataspace.copernicus.eu/). You can sign up for free and have a monthly availability of 10000 credits.\n",
    "- A dataset with valid geometries (Points or Polygons) in lat-lon projection.\n",
    "- Preferably a dataset with unique IDs per sample \n",
    "- A labelled dataset. Not required for the extraction process, but for the following fine-tuning steps.\n",
    "\n",
    "##### EO data extractions\n",
    "In this first step, we extract for each sample in your dataset the required EO time series from CDSE using OpenEO.\n",
    "For running the job, the user should indicate the following job_dictionary fields:\n",
    "\n",
    "```python\n",
    "    job_params = dict(\n",
    "        output_folder=..., # where to save the extracted dataset\n",
    "        input_df=..., # input georeferenced dataset to run the extractions for \n",
    "        start_date=..., # string indicating from which date to extract data  \n",
    "        end_date=..., # string indicating until which date to extract the data \n",
    "        unique_id_column=..., # name of the column in the input_df containing the unique ID of the samples  \n",
    "        composite_window=..., # \"month\" or \"dekad\" are supported. Default is \"dekad\"\n",
    "    )\n",
    "```\n",
    "in particular:\n",
    "- If the `date` information associated with the label is provided, the `start_date` of the time-series is automatically set to 9 months before the date, whereas the `end_date` is set to 9 months after. If `date` is not available, the user needs to manually indicate the desired `start_date` and `end_date` for the extractions. **The indicated period should cover 1 year**. \n",
    "- `composite_window` indicates the time-series granularity, which can be dekadal or monthly. \n",
    "  - `dekad`: each time step in the extracted time series corresponds to a mean-compositing operation on 10-days acquisitions. Accordingly with the start and end date, each month will be covered by 3 time steps which, by default, correspond to the 1st, 11th and 21th of the month. \n",
    "  - `month`: each time step in the extracted time series corresponds to a mean-compositing operation on 30-days acquisitions. Each month will be covered by 1 time step which, by default, correspond to the 1st of the month.\n",
    "\n",
    "The following decadal/monthly time series will be extracted for the indicated time range:\n",
    "\n",
    "- Sentinel-2 L2A data (all bands)\n",
    "- Sentinel-1 VH and VV\n",
    "- Average air temperature and precipitation sum derived from AgERA5\n",
    "- Slope and elevation from Copernicus DEM\n",
    "\n",
    "Presto accepts 1D time-series. Therefore, if Polygons are provided for the extractions, the latter are spatially aggregated in points which will correspond to the centroid lat lon geolocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Run extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "start_date=\"2023-01-01\"\n",
    "end_date=\"2023-12-31\"\n",
    "composite_window=\"dekad\"\n",
    "unique_id_column=\"Field_ID\"\n",
    "input_df=\"/home/giorgia/Private/data/geomaize/correct/Maize_2023_valid.geojson\"\n",
    "output_folder=\"/home/giorgia/Private/data/geomaize/new_extractions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check input data structure \n",
    "gpd.read_file(input_df).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_params = dict(\n",
    "    output_folder=output_folder,\n",
    "    input_df=input_df,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    unique_id_column=unique_id_column,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "extract(generate_input_for_extractions(job_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset will be extracted, it can be loaded with the `load_dataset` function by specifying the path where the `.parquet` files have been downloaded. Moreover, **if** we are dealing with 1 year of data falling in the **same time period**, the following manipulations of the dataset are also possible.\n",
    "\n",
    "- `window_of_interest`: the user can specify a time window of interest out of the whole available time-series. `start_date` and `end_date` should be provided as strings in a list.\n",
    "- `use_valid_time`: the user might want to define the window of interest based on the `date` the label is associated with. If so, also `required_min_timesteps` should be provided\n",
    "- `buffer_window`: buffers the `start_date` and `end_date` by a number of time steps specified with this argument. \n",
    "\n",
    "In the following cell, we load the extracted dataset for 1 year of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Presto datasets initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractions = load_dataset(\n",
    "    output_folder,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "train_df, test_df, val_df = train_test_val_split(\n",
    "    extractions,\n",
    "    uniform_sample_by=unique_id_column,\n",
    "    sampling_frac=0.8,\n",
    "    nmin_per_class=1, # do not change this parameter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up the parameters needed for initializing presto datasets for the specific task:\n",
    "- `num_timesteps`: can be inferred by the max number of the `available_timesteps` \n",
    "- `target_name`: name of the column containing the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution to check for outliers to exclude if needed\n",
    "num_timesteps = extractions.available_timesteps.max()\n",
    "task_type = \"regression\"\n",
    "target_name=\"Yield kg/H\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Initialize the training, validation and test datasets objects to be used for training Presto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ScaleAgDataset(\n",
    "    dataframe=train_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "val_ds = ScaleAgDataset(\n",
    "    dataframe=val_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "test_ds = ScaleAgDataset(\n",
    "    dataframe=test_df,\n",
    "    num_timesteps=num_timesteps,\n",
    "    task_type=task_type,\n",
    "    target_name=target_name,\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Presto Finetuning\n",
    "\n",
    "In this section Presto will be Fine-Tuned in a supervised way for the target downstream task. first we set up the following experiment parameters:\n",
    "\n",
    "- `output_dir` : where to dave the model \n",
    "- `experiment_name` : the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model Hyperparameters\n",
    "models_dir = Path(\"/home/giorgia/Private/data/geomaize/models/\")\n",
    "experiment_name = \"presto-ss-wc-10D-ft-dek-geomaize-lognorm\"\n",
    "model_output_dir = models_dir / experiment_name\n",
    "model_output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the model with finetuning head starting from the pretrained model\n",
    "finetuned_model = finetune_on_task(\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    pretrained_model_path=get_pretrained_model_url(composite_window=composite_window),\n",
    "    output_dir=model_output_dir,\n",
    "    experiment_name=experiment_name,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    max_epochs=100, # max num of training rounds the model should go through\n",
    "    patience=10, # how many epochs to wait for improvement before stopping\n",
    "    lr=1e-3, # learning rate. this paramenter shoul be kept < 1e-3. usually very low (eg 2e-5)\n",
    "    )\n",
    "# save ids to csvs for experiment replication\n",
    "val_df['sample_id'].to_csv(model_output_dir / \"val_sample_ids.csv\", index=False)\n",
    "test_df['sample_id'].to_csv(model_output_dir / \"test_sample_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "finetuned_model = load_finetuned_model(\n",
    "    model_path = model_output_dir / experiment_name,\n",
    "    task_type=task_type,\n",
    ")\n",
    "\n",
    "metrics, preds_original_units, targets_original_units = evaluate_finetuned_model(finetuned_model, test_ds, num_workers=0, batch_size=32)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x=np.arange(len(preds_original_units)), y=preds_original_units, label='preds')\n",
    "plt.scatter(x=np.arange(len(targets_original_units)), y=targets_original_units, label='targets')\n",
    "plt.xticks(ticks=np.arange(len(test_df)), labels=test_df.sample_id.to_list(), rotation=45)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Inference using Fine-Tuned end-to-end Presto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we apply the fine tuned model to generate a yield map on an unseen area. \n",
    "We need to indicate the spatial and temporal extent. The 2 cells below, offer a simple way for the user to provide these information and perform once again the extraction from CDSE of the EO time-series required by Presto. \n",
    "We also need to indicate the `output_dir` of where to save the datacube of the extraction, its `output_filename` and the `composite_window` which will be the same as used for finetuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = ui_map(area_limit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 1 year of data\n",
    "slider = date_slider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"/home/giorgia/Private/data/geomaize/regression\")\n",
    "output_filename = \"inference_area\"\n",
    "inference_file = output_dir / f\"{output_filename}.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_inputs_for_inference(\n",
    "    spatial_extent=map.get_extent(),\n",
    "    temporal_extent=slider.get_processing_period(),\n",
    "    output_path=output_dir,\n",
    "    output_filename=f\"{output_filename}.nc\",\n",
    "    composite_window=composite_window,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the datacube has been extracted, we can perform the inference task using the finetuned model and visualize the predicted map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_file = output_dir / \"inference_area.nc\"\n",
    "mask_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = load_finetuned_model(model_output_dir / experiment_name, task_type=task_type)\n",
    "presto_model = PrestoPredictor(\n",
    "    model=finetuned_model,\n",
    "    batch_size=50,\n",
    "    task_type=task_type,\n",
    "    composite_window=composite_window,\n",
    ")\n",
    "\n",
    "predictions = presto_model.predict(inference_file, mask_path=mask_path)\n",
    "predictions_map = reshape_result(predictions, path_to_input_file=inference_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(prob_map=predictions_map, path_to_input_file=inference_file, task=task_type, ts_index=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
